# ai-verification-duplication-service

**Owner:** @Michal

---

# AI Verification-Duplication Service

The **AI Verification-Duplication Service** uses machine learning to detect fake reports and identify duplicate submissions in RiskRadar. It employs BERT-based models for content analysis and similarity detection, helping maintain data quality and prevent abuse.

---

## ðŸ—ï¸ Architecture

### Core Capabilities

- **Fake Detection** â€“ BERT-based classifier to identify potentially fraudulent reports
- **Duplicate Detection** â€“ Cosine similarity analysis to find duplicate submissions
- **Event Publishing** â€“ Kafka integration for verification results
- **Audit Logging** â€“ Complete tracking of all verification operations

### Technology Stack

- **Language:** Python 3.11
- **Framework:** FastAPI with async/await support
- **ML Models:** PyTorch, Transformers (BERT), scikit-learn
- **Messaging:** aiokafka for asynchronous Kafka operations
- **Validation:** Pydantic models with type safety

### Model Components

- **BERT Model** â€“ Fine-tuned `bert-base-uncased` for text embeddings
- **Fake Detector** â€“ Neural network classifier head
- **Scaler** â€“ Feature normalization (joblib)
- **Duplicate Classifier** â€“ Similarity-based duplicate detection

---

## ðŸ“Š API Endpoints

### Health Check

```http
GET /health
```

**Response:**

```json
{
  "status": "healthy",
  "service": "ai-verification-duplication-service",
  "timestamp": "2024-12-02T19:30:45.123Z",
  "models_loaded": true,
  "kafka_enabled": true
}
```

---

### Verify Report

```http
POST /verify
```

**Description:** Analyzes a report to determine if it's potentially fake.

**Request Body:**

```json
{
  "report_id": "report-789",
  "title": "Dziura w chodniku",
  "description": "ZnalazÅ‚em duÅ¼Ä… dziurÄ™ na chodniku przy ul. GÅ‚Ã³wnej",
  "user_id": "user-123",
  "metadata": {
    "location": "ul. GÅ‚Ã³wna 15"
  }
}
```

**Response:**

```json
{
  "report_id": "report-789",
  "is_fake": false,
  "fake_probability": 0.1234,
  "confidence": "high",
  "explanation": "Report classified as authentic with high confidence"
}
```

**Confidence Levels:**

- `high` â€“ Probability difference > 0.3 from threshold
- `medium` â€“ Probability difference 0.15-0.3 from threshold
- `low` â€“ Probability difference < 0.15 from threshold

---

### Check Duplicate

```http
POST /check-duplicate
```

**Description:** Compares a report against existing reports to detect duplicates.

**Request Body:**

```json
{
  "report_id": "report-new",
  "title": "Dziura w drodze",
  "description": "DuÅ¼a dziura na ulicy GÅ‚Ã³wnej",
  "user_id": "user-123",
  "existing_reports": [
    {
      "id": "report-001",
      "title": "Dziura w drodze ul. GÅ‚Ã³wna",
      "description": "Uszkodzenie nawierzchni"
    }
  ]
}
```

**Response:**

```json
{
  "report_id": "report-new",
  "is_duplicate": false,
  "duplicate_probability": 0.7523,
  "similar_reports": [
    {
      "report_id": "report-001",
      "title": "Dziura w drodze ul. GÅ‚Ã³wna",
      "similarity": 0.7523
    }
  ]
}
```

---

## ðŸ”§ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `PORT` | Service port | `8080` |
| `KAFKA_BROKERS` | Kafka bootstrap servers | `kafka:9092` |
| `KAFKA_CLIENT_ID` | Kafka client identifier | `ai-verification-duplication-service` |
| `KAFKA_ENABLED` | Enable/disable Kafka | `true` |
| `AUDIT_SERVICE_URL` | Audit log service URL | `http://audit-log-service:8080` |
| `AUDIT_ENABLED` | Enable/disable audit logging | `true` |

---

## ðŸ¤– Machine Learning Models

### Model Files

Located in `detector_model_components/`:

- `bert_model_finetuned.pth` â€“ Fine-tuned BERT weights
- `fake_detector_head.pth` â€“ Classification head
- `scaler.joblib` â€“ Feature scaler
- `duplicate_classifier.joblib` â€“ Duplicate detection model

---

## ðŸ“ Audit Events

### Logged Actions

- `service_startup` â€“ Service initialization with model status
- `service_shutdown` â€“ Service shutdown
- `verify_report` â€“ Fake detection performed
- `check_duplicate` â€“ Duplicate check performed

---

## ðŸ”„ Kafka Integration

### Published Topics

- **`verification_events`** â€“ Verification and duplicate check results
- **`notification_events`** â€“ User notifications for fake reports

---

## ðŸš€ Development

### Local Setup

```bash
cd services/ai-verification-duplication-service
pip install -r requirements.txt
uvicorn main:app --reload --port 8080
```

### Docker

```bash
docker build -t ai-verification-duplication-service .
docker run -p 8089:8080 ai-verification-duplication-service
```
